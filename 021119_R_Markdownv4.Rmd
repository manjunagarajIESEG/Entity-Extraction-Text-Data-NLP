---
title: "Concept Extraction on Job Descriptions"
author: "Manju Manjunagaraj, Remo Boesiger and Deborah Kewon"
date: "February 14th, 2019"
output: html_document
---

---
title: "Concept Extraction on Job Descriptions"
output:
  html_document:
    toc: true
    toc_depth: 2
---

---
title: "Concept Extraction on Job Descriptions"
output:
  html_document:
    toc: true
    toc_float: true
---
---
title: "Concept Extraction on Job Descriptions"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

---
title: "Concept Extraction on Job Descriptions"
output:
  html_document:
    toc: true
    number_sections: true
---

## Report {.tabset}

### Introduction

As future data scientists and analysts, we believe that knowing the latest trends of data science job market is a key to employment. Our goal of this project is to have better insights on most in-demand technical skills and positions and the regions and types of companies that are in need of data scientists.We used a raw dataset of 1911 job descriptions extracted from CVs and turned it into a basetable with 1911 observations and 27 variables (for details,please refer to the methodology tab).98% of job descriptions are in English (1871 observations), but on very rare occasions, they are also in French. For pre_processing and text mining, we used the following packages and techniques: tm_map, stringr,openNLP,Regex,SnowballC,slam,RWeka, textcat, cld2, and Matrix.


### Methodology and Results

OpenNLP
<br />
We started with a test run with the openNLP package. However, we quickly realized that these pre-built models will not lead to great results in our particular case. One limitation of these NER models became obvious. If documents such as the job descriptions we were using, are containing many program names and software descriptions it is almost impossible for the system to determine whether an entity is a company or just a skill. For example, if the person was working at Oracle in Redwood City the system should detect Oracle as the company/employer. However, if the person was working with an Oracle database management system it should not detect Oracle as the company/employer but as a skill. With this in mind we changed our strategy from the pre-built models to a dictionary and rule-based approach. Therefore, we created a dictionary with the classifications we want to extract such as the job title the location and the skills. Moreover, we studied the raw text files in order to identify patterns and define rules on how to extract the entities from the unstructured text. As for some extractions we needed special characters we had to abandon some of the pre-processing steps we have seen in class.

Job Titles
<br />
Luckily, most of the job descriptions are starting with the job title. Knowing this, we created a function that checks whether one of the general terms mentioned in our dictionary such as engineer or developer are matched in the unstructured text. If there is a match we split the whole text into two strings right after this term. Finally, we are taking the job title that contains the term mentioned before and whatever is left to it. This approach is fairly simple but catches most of the job titles. However, it has some drawbacks at it is not able to extract multiple job titles in one document.
Based on this approach, we found out that 39%, 20% and 4% of job positions are about developers,engineers and analysts respectively.

Organizations
<br />
As for the organisations, the approach is similar to the job title. Knowing the general position of the location in the offers allowed us to split the unstructured text twice and extract the remaining part. In some cases, the location is just not provided. Therefore, we had to decide that whenever the remaining string is longer than 7 words it is highly unlikely that this is a company name and thus set the value to "missing".

Language
<br />
To detect the language of the job description we tested the two packages textcat and cld2. In our case there was no notable performance difference between the packages. However, as the R-community highlights the increased performance of cld2. While having a look at the results textcat identifies the language for every document but detects strange languages such as rumantsch and catalan which are not true. On the other hand, cld2 gives NA's for ten of the 1911 documents (mostly short descriptions with many special characters) but correctly identifies the language (English or French) for the rest of the documents. Considering the suggestion of the R-community but also given the results we ended up using the cld2 package.

Location
<br />
In order to see which region is in high need of data scientists and analysts, we first reviewed the job descriptions and found the following patterns:1) locations are all in the states 2)city and state are separated by comma 3) city always comes before the state and 4) state has a trailing space meaning that it is located at the end of a line without any characters following it. Based on these findings, We first replaced the punctuations with the spaces and removed all the whitespaces at the end of the states. Once all the texts are seperated by space, we matched the splitted text with the file we created, which includes the names of the states and their abbreviations (ex."CA" California).If the splitted text matches with the state abbreviation on our manually created file, we obtained the position of the text (usually one step before the state as city comes before the state) and assigned it to the city column. For states, we applied the same Regex technique. However, in this case, we did not get the position before the state as we are extracting the state itself. To avoid extracting any random words that have characters of state abbreviations, we decided to keep the state abbreviations in capital letter. According to our model, California,New York and Texas are in high need of data/tech savy professionals, and West Virginia, Montana and HI are in low need.

Skills 
<br />

Evaluation
<br />
To compare the result of our model we created a comparison set by reading 20 random descriptions and manually writing down the results for each of the 23 categories. Comparing this comparison set with the data frame that resulted from our program, we were able to calculate the accuracy. Although the accuracy of the model reaches 91 percent, we have to keep in mind that this score is heavily dependent on the skills section. If we do not consider the skills section, the accuracy drops to 76 percent, which is a more realistic value of the performance.

### Conclusion

As a conclusion, we can say that the model performs well on the given type (job description) of unstructured text. However, it is heavily dependent on the structure of these descriptions. Therefore, further developments or improvements should focus on generalisation.

### Further Steps 
