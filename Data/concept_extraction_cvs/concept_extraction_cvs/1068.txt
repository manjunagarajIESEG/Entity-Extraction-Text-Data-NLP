Big Data Lead / Architect
Capitol One - Reston, VA
Project Description: Capital One is diversified bank that offer a broad array of financial products and services to consumer, small business and commercial clients. Capital One has one of the most widely recognized brand in America. As one of the Nation's top 10 largest banks based on deposit and transaction. Objective ofthe Project was to migration of Teradata Tables to HDFS.As a part of Fill the Lake initiative, I was involved in Data Migration of home loan, crdb, cards data from Historical/Daily data from Teradata/Oracle to HDFS.  Responsibilities: • Creating ETL Process to Move Data from Landing Phase/Key Phase/Split to Hive Tables. • Historical loads (one-time) from Teradata using an unload script built on Teradata Parallel Transporter (TPT) and FastLoad Scripts. • Recurring loads from the Oracle source system using an sFTP pull framework  and Landing of files into the unix edge node. • Registration of the datasets in a metadata registry that controls admittance into Hadoop. • Designed criteria for progressive classification of the datasets within Hadoop (from raw to validated, refined and archived) • Designed the data processing approach within Hadoop using Pig. • Keying to insert keys to support the data lineage based on certain ids. This is done through a java program that looks up the data against a master file • Splitting to shard the dataset into NPI, Credit and Anonymous classifications as per federal regulations. This is accomplished through a python script where the schema of the split is passed as a parameter. • ELT, using Talend Big Data for processing the data. Transformations done include joins, reformats, sorts, if-else conditional transforms etc.  Environment: Cloudera Hue, HDFS, Pig, Hive, Teradata, Oracle 10g and UNIX.
