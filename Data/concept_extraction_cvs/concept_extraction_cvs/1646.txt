Big Data/Hadoop Developer
Cardinal Health - Columbus, OH
Responsibilities: • Worked on Hadoop cluster running CDH4.4. • Worked with highly unstructured and semi structured data of 2 Petabytes in size. • Involved in full life cycle of the project from Design, Analysis, logical and physical architecture modeling, development, Implementation, testing. • Created Hive Tables, loaded transactional data from Teradata using Sqoop. • Developed MapReduce (YARN) jobs for cleaning, accessing and validating the data. • Created and worked Sqoop (version 1.4.3) jobs with incremental load to populate Hive External tables. • Developed optimal strategies for distributing the web log data over the cluster importing and exporting the stored web log data into HDFS and Hive using Sqoop. • Extensive experience in writingPig (version 0.11) scripts to transform raw data from several data sources into forming baseline data. • Implemented HiveGenericUDF's to incorporate business logic into Hive Queries. • Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit duration, most visited page on website. • Integrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Map-Reduce, Pig, Hive, and Sqoop) as well as system specific jobs (such as Java programs and shell scripts) • Creating Hive tables and working on them using Hive QL. • Designed and Implemented Partitioning (Static, Dynamic), Buckets in HIVE. •  Involved in End-to-End implementation of ETLlogic. •  Developed syllabus/Curriculum data pipelines from Syllabus/Curriculum Web Services to HBASE and Hive tables. • Worked on Cluster co-ordination services through Zookeeper. • Monitored workload, job performance and capacity planning using Cloudera Manager. • Involved in build applications using Maven and integrated with CI servers like Jenkins to build jobs. • Exported the analyzed data to the RDBMS using Sqoop for to generate reports for the BI team. • Worked collaboratively with all levels of business stakeholders to architect, implement and test Big Data based analytical solution from disparate sources. • Involved in Agile methodologies, daily scrum meetings, spring planning.  Environment: Hadoop, HDFS, Map Reduce, Hive, Pig, Hbase, Sqoop, Oozie, Maven, Shell Scripting, CDH.
