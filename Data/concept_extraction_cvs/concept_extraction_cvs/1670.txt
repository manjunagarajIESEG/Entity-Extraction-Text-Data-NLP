Sr.Hadoop Developer
Phelps Dodge - Scottsdale, AZ
Responsibilities: • Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka, Pig, Hive and Map Reduce. • Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis. • Real time streaming the data using Spark with Kafka. • Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scale. • Worked within the Apache Hadoop framework, utilizing Twitter statistics to ingest the data from a streaming application program interface (API), automate processes by creating Oozie workflows, and draw conclusions about consumer sentiment based on data patterns found through the use of Hive for external client use. • Worked on building BI reports in Tableau with Spark using SparkSQL. • Developed Hadoop streaming Map/Reduce works using Python. • Worked on debugging, performance tuning of Hive & Pig Jobs. • Worked on Gemfire Distributed caching technology. • Implemented test scripts to support test driven development and continuous integration. • Worked on tuning the performance Pig queries. • Involved in loading data from LINUX file system to HDFS. • Importing and exporting data into HDFS using Sqoop and Kafka. • Good knowledge on building Apache spark applications using python. • Experience working on processing unstructured data using Pig. • Developed Hadoop streaming Map/Reduce works using Python. • Implemented Partitioning, Dynamic Partitions, Buckets in Hive. • Implemented Spark using Scala and SparkSQL for faster testing and processing of data. • Expertise with NoSQL databases like Hbase, Cassandra, DynamoDB (AWS) and MongoDB • Plan, deploy, monitor, and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMWare Vm's as required in the environment. • Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool. • Supported Map Reduce Programs those are running on the cluster. • Gained experience in managing and reviewing Hadoop log files. • Involved in scheduling Oozie workflow engine to run multiple pig jobs. • Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows. • Involved in using HCATALOG to access Hive table metadata from Map Reduce or Pig code. • Continuous monitoring and managing the Hadoop cluster using Cloudera Manager. • Developed Enterprise Lucene/Solr based solutions to include custom type/object modeling and implementation into the Lucene/Solr analysis (Tokenizers/Filters) pipline. • Created custom Solr Query components to enable optimum search matching. • Responsible for developing data pipeline using flume, Sqoop and pig to extract the data from weblogs and store in HDFS. • Proficient work experience with NOSQL, Monod databases. • Developed Hadoop streaming Map/Reduce works using Python. • Administrating Tableau Server backing up the reports and providing privileges to users. • Worked on Tableau for generating reports on HDFS data. • Represented the retrieved results through tableau. • Extracted and updated the data into Monod using Mongo import and export command line utility interface. • Data scrubbing and processing with Oozie. • Developed Pig Latin scripts to extract data from the web server output files to load into HDFS. • Used Python 2.7 and Google App Engine with webapp2 for programming. • Involved in developing Hive DDLs to create, alter and drop tables. • Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts. • Extracted and updated the data into Monod using Mongo import and export command line utility interface. • Used Eclipse and ant to build the application. • Used NoSQL database with Cassandra and Monod.  Environment: Hadoop, HDFS, Pig, Hive, Map Reduce, Sqoop, Spark, AWS EC2, S3, RDS, Kafka, Solr, LINUX, Cloudera, Big Data, Java APIs, Java collection, Gemfire, Python, SQL, NoSQL, Cassandra, Tableau, HBase.
