Big Data Consultant
State Farm Insurance - Bloomington, IL
Project Description: Vision is to provide an Enterprise Hadoop environment to retire existing redundant assets at a reduced per unit cost and enable additional capabilities. Migrate data from all existing enterprise analytical environments and sunset existing assets. Enable capabilities (Both existing and new) as necessary to meeting analytical needs. Enable governance to ensure risk and compliance is met in the Hadoop ecosystem. Support data management and control redundancy of data in the new ecosystem.  Responsibilities: • Sqoop to extract data from IBM DB2 and DB2 on Zos databases to Hive and HDFS. • Data transformation using Hive. • Data formats like Avro and Parquet. • Compression Codecs like snappy, GZip, BZip and LZ4. • XML to avro and XML to parquet conversion using open source projects from github. • Data compression testing. • Data governance using Cloudera navigator. • Data ingestion and transformation from nCluster Aster database using python and hive. • Sqoop Benchmark testing with 1 to n mappers for small, medium and large tables. • Created hive backed reporting tables for BI tools like Cognos and Qlkview. • Source control using GitLab. • Scheduling jobs using Oozie and shell scripts. • Performance tuning in production Hive scripts • User credentials encryption in sqoop using bouncy castle. • Partitioning and bucketing in Hive tables for query performance and data sampling. • Query builders using Hive, impala, hive CLI and Beeline editor. • Source database editors like IBM Data base studio, DB2 CMD and Aster ACT.  Environment: CDH 5.4.3 Development: 240 TB, 6 Nodes Infrastructure: 240 TB, 4 nodes Production: 2400 TB, 20 Nodes
