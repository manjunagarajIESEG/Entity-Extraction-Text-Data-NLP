Hadoop Data Engineer
Directv
DirecTV provides television and audio services to subscribers through satellite transmissions. Services include the equivalent of many local television stations, broadcast television networks, subscription television services, satellite radio services, and private video services. Subscribers have access to hundreds of channels, so its competitors are cable television service and other satellite-based services. Project: Viewer RECOMMENDATIONS Responsibilities Worked on Planning, Analysis, Design, Implementation, Maintenance of a Customer Facing Big Data application involving umpteen jobs across different members of Hadoop Eco System. Dataset: Customer Viewing History Data Ingestion: • Experience of developing batch processing framework to ingest data into HDFS, Hive and Hbase. • Written tokenization [Security Practice] job to desensitize data on local clusters before ingesting into the AWS environment. • Imported customer account data from RDBMS (SQLSERVER MS) using Sqoop to enrich viewing history data available on HDFS. PIG: • Developed embedded Pig Latin scripts in Python in compile, bind, run model for enabling control flow and used Pig Unit for rapid prototyping and testing scripts in local mode. • Developed FILTER and EVAL UDF's in java for PIG. • Integrated native Map-Reduce jobs in PIG data pipeline using the MAPREDUCE Command. • Fine Tuned Pig Job's performance parameters along with native map-reduce parameters to avoid excessive disk spills, enabled temp file compression between jobs in the data pipeline to handle production size data in a multi-tenant cluster environment • Implemented SKEWED and REPLICATED joins. HIVE: • Generated ad-hoc reports using Hive/Impala to validate customer viewing history and debug issues in production. • Hands on writing complex Hive queries involving external dynamic partitioned on date Hive Tables which stores rolling window time-period user viewing history • Used custom mapper/reducer Python script in Hive using Transform and developed custom UDFs. Experience of performance tuning hive scripts, pig scripts, MR jobs in production environment by altering job parameters. • Used SequenceFileFormat and Parquet to store data on Hive tables. Python: • Used Jython scripts for MR, Pig, Hive job control and validation purpose. • Worked on developing a python deployment script to build, release, deploy and update configurations of the Hadoop Application. • Maintained Jython scripts to create, populate, fetch and delete data from Hbase using HBASE API. BASH Scripting • Proficient in bash scripting and written scripts to monitor and debug jobs in production. • Worked on setting up cron jobs to schedule jobs to get data to Hadoop gateway from ftp servers. • Worked on migration of jobs from CDH 4.6 to CDH 5.3. • Developed wrappers around Hadoop jobs[Hive/Java M-R] in shell for job control and validation. Map Reduce: • Developed Summarization, Filtering, Data Organization, Join Patterns using Map/Reduce Jobs in Java. • Implemented the Tool Interface in the MR driver code to have configuration picked up dynamically. • Experience in chaining MR jobs and using SequenceFileFormat along with Snappy Compression in generated intermediate data generated. Oozie • Hands on experience of building data pipelines using Oozie workflows, coordinators and bundles across dependent/independent jobs accessing HDFS, Hive, Impala, shell, Hbase with active use of EL functions, constants. • Implemented SequenceFileFormat with snappy compression to store data between jobs in the workflow. • Experience setting up asynchronous inter cluster Oozie workflows. Spark: • POCs on moving existing Pig Latin jobs to Spark using Python. SOLR • POC on setting batch indexing process for indexing data into SOLR servers[distributed]. Avro: • POC on using Avro to store  datasets across projects. Data Analytics: • Developed Pig Latin Script with custom UDFs to calculate What-is-Trending Analytic Job in the current viewer watch records based on configurable rules & days and push data to Hbase in JSON format. • Written and maintained MR job to calculate the Last Action Analytic job calculating the latest activity of the user based on category, sub-category and genres and push result to Hbase in JSON format. Hadoop Architecture: • Excellent understanding of Hadoop1 and Hadoop2(Yarn) architecture with extensive knowledge on functioning of Resource Manager, Node Manager, Journal Node, Application Master, Container. • Worked with Admin to setup stand by Job Tracker and enable HA feature on CDH 4.6 and • Extensive experience of cluster sizing (HDFS & HBASE) in production environment. • Good Working Knowledge of Cloudera Manager for monitoring and managing the Hadoop cluster using Cloudera Manager. • Worked with Hadoop Admin teams to install operating system, Hadoop updates, patches, version upgrades as required. • Knowledge on AWS environment and different type of node families used to setup clusters. HBASE: • Experience in sizing of production cluster based on data stored on HBASE. • Used Hbase tables to capture all job statistics and maintained job history table based on batch numbers. • Worked on split architecture involving HDFS and HBASE on different clusters and running inter cluster operations. • Worked on performance testing HBASE using YCSB (Yahoo Cloud Servicing Benchmark) by setting it up on Cloudera Hadoop Cluster. • Hands on experience with hbase shell, CLI to perform DDL/DML operations on Hbase tables. • Extensive experience of generating data in HFiles using HFileOutputFormat and loading it into active Hbase cluster using the Bulk load API. Data Monitoring: • Developed a generic HDFS monitoring tool which sends alerts to stake holders when data stales. Environment AWS, Hadoop, MapReduce, HDFS, Hive, Pig, Python Impala, Spark(Python), Java, SQL, CDH 4.6-5.3, Sqoop, Flume, Oozie, Java (jdk 1.6), Eclipse, SequenceFileFormat, Parquet, Avro
