Linux Software Engineer
Translational Genomics Research Institute (TGen) - Phoenix, AZ
Responsibilities Dell's Kid's Cloud  A project sponsored by Dell Computer to investigate pediatric brain cancer. Parents, physicians and scientists from the Van Andel Research Institute (VARI) and TGen teamed to launch this clinical trial investigation. The trial is based on research from a group of collaborating investigators who are developing a personalized medicine process that is intended to permit near “real time” processing of information on patient tumors and prediction of best drugs for a specific patient. There were several aspects to the study. One aspect was to reduce the total time it takes to do the genetic sequencing. The process of sequencing generates more than 200 billion measurements per patient that must be analyzed, shared and stored.   My role in reducing the time for sequencing was to implement a BcBio-nextgen pipline. (See the listing below for details)  Another aspect was to develop a VMware cloud where the results of the individual patients sequencing could be displayed. This Linux web based application is know as LightBox. I was responsible for the dev-ops of the VMware cloud, MySQL and Mongo databases, Perl scripts and system integration and system administration of this small cloud base application. One of the focuses of my development was on the conversion of a Linux C++/Qt bit torrent application to be used for securely sharing genetic data. The bit torrent application was developed in Centos, C/C++(g++), Linux thread, with object oriented design and development. The VMware cloud was developed with VMware 5.0, Vsphere, Centos, MySQL, Mongo.  Bcbio-nextgen  Integrate best-practice pipeline for automated analysis of high throughput sequencing data for neurobiology lab. Bcbio-nextgen is an open source Python toolkit that provides automated high throughput sequencing analysis. The pipeline is for taking sequencing results from an Illumina sequencer, converting them to standard Fastq format, aligning to a reference genome, doing SNP calling, and producing a summary. I worked directly with lab bioinformatics personal to install, integrate, build and test this new pipeline. The pipeline is build with many different open source tools. These tools where developed in many different programming languages. A few of the one I dealt with were, PERL, Python, C/C++, Bash. The pipeline was built and ran on a Centos HPC cluster. Using Torque and Maui job queuing.  Large Data Archive Develop a Big Data archive application to archive and restore large genomic data. TGen sequences large volumes of DNA and RNA. Genetic sequencing produces data in the 100s of petabytes. The data is too large for normal backup and recovery tools. Normal copy and rsync take far too long due to the amount of the data. An Archive tool to off load large parts of this data was required. The Archive tool utilizing an HPC cluster copies data to an /arc file system (Sun StorEdge SAM-FS) where the data is stored to tape. The Archive tool is a Linux command line application. The archive reads the data and breaks it up into SLURM jobs. Once the jobs are created. SLURM fires the jobs off on to a list of HPC compute nodes that handle the actually processing. The processing included coping the data files, gathering of metadata that is stored in MySQL database. The archive is developed in PERL, using MySQL, Mongo, Slurm, Subversion, parallelism, Linux threads, Centos, VMware, HPC, Sun StorEdge SAM-FS..
