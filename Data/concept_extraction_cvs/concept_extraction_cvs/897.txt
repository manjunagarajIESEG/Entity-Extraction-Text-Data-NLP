Sr Hadoop Developer
Credit Suisse, NewYork
Responsibilities: • Developed simple and complex MapReduce programs in Java for Data Analysis on different data formats • Developed MapReduce programs that filter bad and un-necessary claim records and find out unique records based on account type • Processed semi, unstructured data using Map Reduce programs • Implemented Daily Cron jobs that automate parallel tasks of loading the data into HDFS and pre-processing with Pig using Oozie co-ordinator jobs • Implemented custom DataTypes, InputFormat, RecordReader, OutputFormat, RecordWriter for MapReduce computations • Worked on CDH4 cluster on CentOS. • Successfully migrated Legacy application to Big Data application using Hive/Pig/HBase in Production level • Transformed date related data into application compatible format by developing apache Pig UDFs • Developed MapReduce pipeline for feature extraction and tested the modules using MRUnit • Optimized MapReduce jobs to use HDFS efficiently by using various compression mechanisms • Creating Hive tables, loading with data and writing Hive queries which will run internally in MapReduce way • Responsible for performing extensive data validation using Hive • Implemented Partitioning, Dynamic Partitions and Bucketing in Hive for efficient data access • Worked on different set of tables like External Tables and Managed Tables • Used Oozie workflow engine to run multiple Hive and Pig jobs • Involved in installing and configuring Hive, Pig, Sqoop, Flume and Oozie on the Hadoop cluster. • Involved in designing and developing non-trivial ETL processes within Hadoop using tools like Pig, Sqoop, Flume, and Oozie • Used DML statements to perform different operations on Hive Tables • Developed Hive queries for creating foundation tables from stage data • Used Pig as ETL tool to do transformations, event joins, filter and some pre-aggregations • Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior • Implemented business logic by writing Pig UDFs in Java and used various UDFs from Piggybanks and other sources • Working with Apache Crunch library to write, test and run HADOOP MapReduce pipeline jobs • Involved in joining and data aggregation using Apache Crunch • Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose • Involved in writing, testing, and running MapReduce pipelines using Apache Crunch • Queried and analyzed data from Datastax Cassandra for quick searching, sorting and grouping • Developed Mapping document for reporting tools  Environment: Apache Hadoop, HDFS, MapReduce, Apache Crunch, Java (jdk1.6), MySQL, DB Visualizer, Linux, Sqoop, Apache Hive, Apache Pig
