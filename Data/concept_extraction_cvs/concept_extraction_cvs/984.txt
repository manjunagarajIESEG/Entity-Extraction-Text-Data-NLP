Hadoop Developer/Big Data Analyst
Catalina Marketing - Saint Petersburg, FL
One of the Key Projects: D-Log Data Processing Using Hadoop-MapReduce Framework The main aim of this project was to extract business values from the large volume of messy unstructured/semi-structured/structured data. D-Log burst and shell scripts were used to load the data onto HDFS/Netezza Database. Iterative analytical process (research questions -> data wrangling <-> feature extraction <-> model development <-> model evaluation -> interactive visualization to answer questions) was used with appropriate Statistical techniques and Machine learning algorithms using custom Map Reduce programs, Hive, Pig, Python, and R. Also, for the OLAP type of operational usages, a query-utility was developed to extract meaningful information from nicely organized Hive external tables. The ultimate delivery of the project was a successful web-based Data Product using R Shiny.  Technologies: C++, Hadoop Cluster (Hortonworks), Java 1.7, MySQL, Netezza IBM Database, hive-0.10, hue-2.5, oozie-3.3.2, pig-0.11, sqoop-1.4.3, sqoop2-1.99, zookeeper-3.4.5, Red hat Linux, Unix scripting, log4j, JUnit testing & Maven Kerberos, RStudio Server.  Roles and Responsibilities: • Process D-log data using Mapreduce code, Pig, Hive external tables, python and shell scripting. • Validating data size in Netezza and Hadoop by executing shell scripts. • Submitting job against the development directory data and checking background process. • Managing jobs on Hadoop cluster - send email to Hadoop user group by copying job summary from the trail of log file. • Creating and submitting job using HUE. • Extracting sample data from external Hive tables for the models using Stratified Sampling technique. • Developing Machine Learning driven models using R and Spark MLlib. • Development of actionable web-based Data Products using R Shiny.  Now working with Airisdata a client of Catalina.  Roles and Responsibilities: ●    Spark streaming and putting Data into  HDFS and Cassandra DB ●   Analyze  RDD and interactive visualization with Graphx ●   Taking Data from Different sources and create data pipeline Source to Kafka to HDFS to Hbase/    Cassandra DB ● Finally, Analyze the Data using Spark (ML Library) and R Shiny Package. ● Log data Streaming and processing
